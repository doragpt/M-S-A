提供されたコード（app.py, aggregated_data.py, database.py, main.py, page_helper.py）を基に、指定された機能（店舗稼働状況、店舗履歴グラフ、時間帯別分析、エリア統計、ジャンルランキング、人気店舗ランキング）が対応するエンドポイントで正しく動作するかを解析し、問題点と修正案を提案します。
機能と対応エンドポイントの解析
1. 店舗稼働状況
目的: 各店舗の最新稼働状況（スタッフ数、稼働率など）を表示。
対応エンドポイント: /api/data
動作解析:
良い点:
最新レコードを取得（WITH LatestTimestamps サブクエリ）。
ページネーション対応（page, per_page）。
稼働率や総計を計算し、JSTでタイムスタンプを整形。
問題点:
SQLite接続エラー時のフォールバックが簡易的で、再試行がない。
format_store_status に依存し、不正データでエラーが発生する可能性。
キャッシュ（60秒）が短すぎて頻繁な更新が必要。
修正案:
再試行ロジック追加:
python
retries = 3
for attempt in range(retries):
    try:
        conn = get_db_connection()
        all_results = list(conn.execute(query).fetchall())
        conn.close()
        break
    except Exception as e:
        app.logger.error(f"試行{attempt+1}/{retries} 失敗: {e}")
        if attempt == retries - 1:
            return jsonify(fallback_response), 200
        time.sleep(1)
キャッシュ期間延長: timeout=300（5分）に変更。
python
@cache.memoize(timeout=300)
def api_data():
2. 店舗履歴グラフ
目的: 店舗の稼働状況の時系列データを表示。
対応エンドポイント: /api/history, /api/history/optimized
動作解析:
良い点:
フィルタリング（store, start_date, end_date）が可能。
/api/history/optimized で大量データ時のダウンサンプリング対応。
問題点:
SQLiteフォールバック時のSQLが簡易的で、日付フィルタが不正確。
ダウンサンプリングのサンプル数が固定（100ポイント）で柔軟性不足。
エラー時のレスポンスが500と200で不統一。
修正案:
SQLiteクエリの改善:
python
if start_date:
    sql_query += " AND timestamp >= ?"
    params.append(f"{start_date} 00:00:00")
if end_date:
    sql_query += " AND timestamp <= ?"
    params.append(f"{end_date} 23:59:59")
動的サンプリング:
python
sample_rate = max(1, data_len // min(100, total_count // len(store_names)))
レスポンスコード統一: 全て200に。
python
return jsonify({...}), 200
3. 時間帯別分析
目的: 時間帯ごとの平均稼働率を表示。
対応エンドポイント: /api/hourly-analysis
動作解析:
良い点:
24時間分のデータをゼロ埋めで返す。
店舗指定（store）が可能。
問題点:
aggregated_data.py の calculate_hourly_average とロジックが重複。
エラー時のデータが空で、メタ情報不足。
修正案:
ロジック統合:
python
from aggregated_data import AggregatedData
@app.route('/api/hourly-analysis')
@cache.memoize(timeout=600)
def api_hourly_analysis():
    store = request.args.get('store')
    conn = get_db_connection()
    hourly_data = AggregatedData.calculate_hourly_average(conn, store)
    conn.close()
    return jsonify({
        "data": hourly_data,
        "meta": {"store": store or "全店舗", "count": len(hourly_data), "current_time": datetime.now(JST).strftime('%Y-%m-%d %H:%M:%S %Z%z')}
    })
エラー時のメタ情報追加:
python
return jsonify({
    "data": [],
    "meta": {"error": str(e), "message": "時間帯別分析エラー", "current_time": datetime.now(JST).strftime('%Y-%m-%d %H:%M:%S %Z%z')}
}), 200
4. エリア統計
目的: エリアごとの店舗数と平均稼働率を表示。
対応エンドポイント: /api/area-stats
動作解析:
良い点:
最新データをサブクエリで取得。
稼働率計算が適切。
問題点:
aggregated_data.py の calculate_area_statistics と重複。
エラー時のレスポンスが500で統一性なし。
修正案:
ロジック統合:
python
from aggregated_data import AggregatedData
@app.route('/api/area-stats')
@cache.memoize(timeout=600)
def api_area_stats():
    conn = get_db_connection()
    results = AggregatedData.calculate_area_statistics(conn)
    conn.close()
    formatted_results = [
        {'area': r['area'] or '不明', 'store_count': r['store_count'], 'avg_rate': round(float(r['avg_rate'] or 0), 2)}
        for r in results
    ]
    return jsonify({
        "data": formatted_results,
        "meta": {"count": len(formatted_results), "current_time": datetime.now(JST).strftime('%Y-%m-%d %H:%M:%S %Z%z')}
    })
レスポンス統一:
python
return jsonify({"data": [], "meta": {"error": str(e), ...}}), 200
5. ジャンルランキング
目的: 業種内のジャンル別平均稼働率ランキングを表示。
対応エンドポイント: /api/ranking/genre
動作解析:
良い点:
biz_type フィルタ対応。
データなし時にダミーデータを返す。
問題点:
aggregated_data.py の calculate_genre_ranking と重複。
サンプル数の閾値（store_count >= 2）がハードコード。
修正案:
ロジック統合:
python
@app.route('/api/ranking/genre')
@cache.memoize(timeout=600)
def api_genre_ranking():
    biz_type = request.args.get('biz_type')
    conn = get_db_connection()
    results = AggregatedData.calculate_genre_ranking(conn, biz_type)
    conn.close()
    data = [
        {"genre": r['genre'] or "不明", "store_count": r['store_count'], "avg_rate": round(float(r['avg_rate'] or 0), 1)}
        for r in results
    ]
    return jsonify({
        "data": data or [{"genre": "該当なし", "store_count": 0, "avg_rate": 0.0}],
        "meta": {"biz_type": biz_type or "全業種", "count": len(data), "current_time": datetime.now(JST).strftime('%Y-%m-%d %H:%M:%S %Z%z')}
    })
閾値の柔軟性:
python
min_samples = request.args.get('min_samples', 2, type=int)
query += f" HAVING store_count >= {min_samples}"
6. 人気店舗ランキング
目的: 期間別の人気店舗ランキングを表示。
対応エンドポイント: /api/ranking/popular
動作解析:
良い点:
期間（daily, weekly, monthly, all）対応。
biz_type フィルタ可能。
問題点:
aggregated_data.py の calculate_store_ranking と部分重複。
エラー時のダミーデータがない。
修正案:
エラー時のダミーデータ:
python
except Exception as e:
    app.logger.error(f"人気店舗ランキング取得エラー: {e}")
    return jsonify({
        "data": [{"rank": 1, "store_name": "データなし", "avg_rate": 0.0, "sample_count": 0, "biz_type": "不明", "genre": "不明", "area": "不明"}],
        "meta": {"error": str(e), "message": "人気店舗ランキングエラー", "current_time": datetime.now(JST).strftime('%Y-%m-%d %H:%M:%S %Z%z')}
    }), 200
集計テーブル活用:
calculate_store_ranking を使用せず、既存テーブルから直接取得（現状のままでもOK）。
修正案のまとめ
共通修正
タイムゾーン: グローバル変数 JST = pytz.timezone('Asia/Tokyo') を app.py で定義し、全エンドポイントで使用。
レスポンス統一: エラー時は常に 200 と {"data": [], "meta": {"error": "..."}} 形式に。
エラーハンドリング: 再試行ロジックを追加し、ログに詳細を記録。
エンドポイントごとの修正
/api/data:
再試行追加、キャッシュ300秒。
/api/history, /api/history/optimized:
SQLiteクエリ改善、動的サンプリング、レスポンス200統一。
/api/hourly-analysis:
AggregatedData 統合、エラー時のメタ情報追加。
/api/area-stats:
AggregatedData 統合、レスポンス200統一。
/api/ranking/genre:
AggregatedData 統合、閾値パラメータ追加。
/api/ranking/popular:
エラー時のダミーデータ追加。
コード例（/api/data の修正）
python
JST = pytz.timezone('Asia/Tokyo')

@app.route('/api/data')
@cache.memoize(timeout=300)
def api_data():
    from page_helper import format_store_status
    app.logger.info("API /api/data 開始")
    now_jst = datetime.now(JST)
    refresh = request.args.get('refresh', '0') == '1'
    if refresh:
        cache.delete_memoized(api_data)

    page = request.args.get('page', 1, type=int)
    per_page = min(request.args.get('per_page', 20, type=int), 100)

    retries = 3
    for attempt in range(retries):
        try:
            conn = get_db_connection()
            query = """
            WITH LatestTimestamps AS (
                SELECT store_name, MAX(timestamp) as max_timestamp
                FROM store_status
                GROUP BY store_name
            )
            SELECT s.*
            FROM store_status s
            JOIN LatestTimestamps lt ON s.store_name = lt.store_name AND s.timestamp = lt.max_timestamp
            """
            all_results = list(conn.execute(query).fetchall())
            conn.close()
            break
        except Exception as e:
            app.logger.error(f"試行{attempt+1}/{retries} 失敗: {e}")
            if attempt == retries - 1:
                return jsonify({"data": [], "meta": {"error": str(e), "current_time": now_jst.strftime('%Y-%m-%d %H:%M:%S %Z%z')}}), 200
            time.sleep(1)

    formatted_data = [format_store_status(item, JST) for item in all_results]
    total_count = len(formatted_data)
    start_idx = (page - 1) * per_page
    paged_data = formatted_data[start_idx:start_idx + per_page]

    return jsonify({
        "data": paged_data,
        "meta": {
            "total_count": total_count,
            "page": page,
            "per_page": per_page,
            "total_pages": ceil(total_count / per_page),
            "has_next": start_idx + per_page < total_count,
            "has_prev": page > 1,
            "current_time": now_jst.strftime('%Y-%m-%d %H:%M:%S %Z%z')
        }
    })
結論
これらの修正により、全てのエンドポイントが統合ダッシュボードで期待通りに機能します。具体的には：
店舗稼働状況: 最新データが正確に表示。
店舗履歴グラフ: 時系列データが適切に取得可能。
時間帯別分析: 24時間データが一貫して表示。
エリア統計: エリアごとの統計が安定。
ジャンルランキング: 柔軟なランキング表示。
人気店舗ランキング: 期間別ランキングが確実。
追加の質問や具体的な実装支援が必要ならお知らせください！